{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ac1842",
   "metadata": {},
   "source": [
    "# IMDB Movie Rating Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014aacf",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a315c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2717c559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Salma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc1c9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"movie.csv\"\n",
    "Movie_Data = pd.read_csv(file_path)\n",
    "Movie_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1f3772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    40000 non-null  object\n",
      " 1   label   40000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "Movie_Data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea512cc",
   "metadata": {},
   "source": [
    "## Apply the preprocessing techniques to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50718db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocesses a string of words by removing stopwords, punctuation, digits, and applying lemmatization.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input string of words\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed string of words\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lem = WordNetLemmatizer()\n",
    "\n",
    "        # Tokenize the text to tokens\n",
    "        tokens = word_tokenize(text)\n",
    "    \n",
    "        # Apply lower casing to each token\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "        # Remove the stop words from the text\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "        # Remove The Punctuation\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "        # Remove digits \n",
    "        tokens = [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "        # Apply lemmatization\n",
    "        lemmatized_tokens = [lem.lemmatize(token) for token in tokens]\n",
    "    \n",
    "        return \" \".join(lemmatized_tokens)  \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occured during preprocessing\")\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002ae0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grew b watching loving thunderbird mate school...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>put movie dvd player sat coke chip expectation...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people know particular time past like feel nee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even though great interest biblical movie bore...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im die hard dad army fan nothing ever change g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  grew b watching loving thunderbird mate school...      0\n",
       "1  put movie dvd player sat coke chip expectation...      0\n",
       "2  people know particular time past like feel nee...      0\n",
       "3  even though great interest biblical movie bore...      0\n",
       "4  im die hard dad army fan nothing ever change g...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the Dataset\n",
    "Movie_Data['text'] = Movie_Data['text'].apply(lambda text: preprocessing(text))\n",
    "Movie_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98326eb3",
   "metadata": {},
   "source": [
    "## Convert the text to Words vectors using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b803ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c97f911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe model\n",
    "model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfcbb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vectorize(text):\n",
    "    \"\"\"\n",
    "    Converts a cleaned string of words into a word vector of 100 dimensions.\n",
    "\n",
    "    Args: \n",
    "        text (str): Cleaned string of words\n",
    "    Returns:\n",
    "        numpy.ndarray: Word vector of 100 dimensions for the string of words\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        vectors = []\n",
    "        tokens = word_tokenize(text)\n",
    "        for token in tokens:\n",
    "            if token in model:\n",
    "                vectors.append(model[token])\n",
    "            \n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0) \n",
    "        else:\n",
    "            return np.zeros(100)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during text vectorization: {e}\")\n",
    "        return np.zeros(100)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2712dfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grew b watching loving thunderbird mate school...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.20546971, 0.05887542, 0.09890097, -0.023451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>put movie dvd player sat coke chip expectation...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.18131112, 0.20964852, 0.047303382, -0.11553...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people know particular time past like feel nee...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.08031409, 0.04627317, 0.09079919, 0.0920195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even though great interest biblical movie bore...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.09096001, 0.006144219, 0.034156606, -0.0272...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im die hard dad army fan nothing ever change g...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.10691197, 0.13346525, 0.056612603, -0.04111...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  grew b watching loving thunderbird mate school...      0   \n",
       "1  put movie dvd player sat coke chip expectation...      0   \n",
       "2  people know particular time past like feel nee...      0   \n",
       "3  even though great interest biblical movie bore...      0   \n",
       "4  im die hard dad army fan nothing ever change g...      1   \n",
       "\n",
       "                                         text_vector  \n",
       "0  [0.20546971, 0.05887542, 0.09890097, -0.023451...  \n",
       "1  [0.18131112, 0.20964852, 0.047303382, -0.11553...  \n",
       "2  [0.08031409, 0.04627317, 0.09079919, 0.0920195...  \n",
       "3  [0.09096001, 0.006144219, 0.034156606, -0.0272...  \n",
       "4  [0.10691197, 0.13346525, 0.056612603, -0.04111...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the text in the Dataset\n",
    "Movie_Data['text_vector'] = Movie_Data['text'].apply(lambda text: text_vectorize(text))\n",
    "Movie_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a020bb8",
   "metadata": {},
   "source": [
    "## LSTM - Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a009d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52d5b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'text_vector' to numpy array\n",
    "X = np.stack(Movie_Data['text_vector'].values)\n",
    "# Reshape the input data to 3D\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "y = Movie_Data['label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc95b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model():\n",
    "    \"\"\"\n",
    "    Create an LSTM model with 128 internal units.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        model = keras.Sequential()\n",
    "        # Add a LSTM layer with 128 internal units.\n",
    "        model.add(layers.LSTM(128, input_shape=(100,1)))\n",
    "        # Add a Dense layer with 32 units.\n",
    "        model.add(layers.Dense(32, activation='relu'))\n",
    "        # Add a Dense Layer with 16 units.\n",
    "        model.add(layers.Dense(16, activation='relu'))\n",
    "        # Add a Dense layer with 1 unit.\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured during LSTM model creation\")\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e19098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               66560     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,233\n",
      "Trainable params: 71,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the LSTM model\n",
    "lstm_model = LSTM_Model()\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d60482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13864f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 106s 2s/step - loss: 0.6833 - accuracy: 0.5687 - val_loss: 0.6650 - val_accuracy: 0.6094\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 101s 2s/step - loss: 0.6412 - accuracy: 0.6393 - val_loss: 0.6366 - val_accuracy: 0.6320\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 101s 2s/step - loss: 0.6325 - accuracy: 0.6468 - val_loss: 0.6324 - val_accuracy: 0.6436\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 101s 2s/step - loss: 0.6341 - accuracy: 0.6441 - val_loss: 0.6352 - val_accuracy: 0.6379\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 100s 2s/step - loss: 0.6303 - accuracy: 0.6505 - val_loss: 0.6343 - val_accuracy: 0.6410\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 100s 2s/step - loss: 0.6282 - accuracy: 0.6517 - val_loss: 0.6286 - val_accuracy: 0.6479\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 101s 2s/step - loss: 0.6208 - accuracy: 0.6596 - val_loss: 0.6275 - val_accuracy: 0.6510\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 101s 2s/step - loss: 0.6168 - accuracy: 0.6649 - val_loss: 0.6163 - val_accuracy: 0.6649\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 101s 2s/step - loss: 0.6124 - accuracy: 0.6707 - val_loss: 0.6272 - val_accuracy: 0.6497\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 100s 2s/step - loss: 0.6057 - accuracy: 0.6753 - val_loss: 0.6026 - val_accuracy: 0.6766\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 101s 2s/step - loss: 0.6129 - accuracy: 0.6663 - val_loss: 0.6180 - val_accuracy: 0.6618\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 100s 2s/step - loss: 0.6046 - accuracy: 0.6765 - val_loss: 0.6034 - val_accuracy: 0.6773\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 100s 2s/step - loss: 0.5939 - accuracy: 0.6858 - val_loss: 0.6080 - val_accuracy: 0.6743\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 75s 1s/step - loss: 0.5960 - accuracy: 0.6836 - val_loss: 0.6008 - val_accuracy: 0.6817\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 62s 985ms/step - loss: 0.5931 - accuracy: 0.6885 - val_loss: 0.5903 - val_accuracy: 0.6905\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 61s 962ms/step - loss: 0.5823 - accuracy: 0.6948 - val_loss: 0.5899 - val_accuracy: 0.6913\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 64s 1s/step - loss: 0.5892 - accuracy: 0.6896 - val_loss: 0.6002 - val_accuracy: 0.6845\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 63s 1s/step - loss: 0.5808 - accuracy: 0.6961 - val_loss: 0.5848 - val_accuracy: 0.6955\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 64s 1s/step - loss: 0.5780 - accuracy: 0.6973 - val_loss: 0.5886 - val_accuracy: 0.6852\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 63s 1s/step - loss: 0.5751 - accuracy: 0.7000 - val_loss: 0.5832 - val_accuracy: 0.6959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e1008918d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "lstm_model.fit(X_train, y_train, batch_size=512, epochs=20, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88fc53d",
   "metadata": {},
   "source": [
    "# Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab30dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text):\n",
    "    \"\"\"\n",
    "    Predicts the label for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text to classified.\n",
    "        \n",
    "    Returns:\n",
    "        string: predicted label (positive or negative).  \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        preprocessed_text = preprocessing(text)\n",
    "        vector = text_vectorize(preprocessed_text)\n",
    "        vector = np.expand_dims(vector, axis=0) # Add batch dimension\n",
    "        \n",
    "        prediction = int(np.round(lstm_model.predict(vector)[0][0]))\n",
    "\n",
    "        \n",
    "        if prediction == 1:\n",
    "            label = \"Positive\"\n",
    "        else:\n",
    "            label = \"Negative\"\n",
    "            \n",
    "        return label    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occured during label prediction: {e}\")\n",
    "        return -1  # Return -1 on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53ae0dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Predicted Label: Positive\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"This is a great movie!\"\n",
    "predicted_label = prediction(text)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518506f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8da14",
   "metadata": {},
   "source": [
    "In this task, I utilized the IMDB Movie rating sentiment analysis dataset. The dataset underwent preprocessing steps to enhance its quality. These steps included the removal of stopwords, punctuations, and digits, as well as converting the words to lowercase and applying lemmatization using the NLTK Library.\n",
    "\n",
    "To represent the words in vector form, I employed the word2vec technique. Specifically, I utilized the Glove model, which provides vectors of 100 dimensions. This conversion facilitated the utilization of the data in the subsequent model.\n",
    "\n",
    "Next, I partitioned the dataset into an 80% training set and a 20% testing set. The training set was utilized to train an LSTM model with 20 epochs.\n",
    "\n",
    "To evaluate the performance of the model, I employed the accuracy metric, which is appropriate for this binary classification problem. The model was trained using the binary cross-entropy loss function and the Adam optimizer.\n",
    "\n",
    "Upon completion of the training process, the LSTM model achieved a training loss of 0.5751 and an accuracy of 70%. For the testing data, the model obtained a loss of 0.5832 and an accuracy of 69%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c6d89",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c85519",
   "metadata": {},
   "source": [
    "As future work to improve the model's performance, several approaches can be considered. Firstly, incorporating a larger and more diverse dataset during training can help the model better capture the nuances and variations in sentiment. Additionally, exploring alternative embedding techniques like BERT or ELMo, or even introducing an embedding layer within the deep learning model, may yield improved performance.\n",
    "\n",
    "Adjusting the model's parameters, such as the number of layers, hidden units, and activation functions, can also be explored to optimize performance. Furthermore, leveraging pre-trained models, such as using a pre-trained language model for transfer learning, has the potential to enhance the model's capabilities.\n",
    "\n",
    "It is worth noting that sentiment analysis faces challenges due to the diverse nature of human expression and language use. To address this, incorporating a lexicon-based approach could aid in detecting slang words or expressions that the classifier may struggle to analyze accurately.\n",
    "\n",
    "Continuing to iterate and refine the model based on careful analysis of errors and misclassifications, along with incorporating domain-specific knowledge, can contribute to further performance improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
